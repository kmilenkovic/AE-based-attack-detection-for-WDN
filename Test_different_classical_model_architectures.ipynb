{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xP-0_OtioUEf"
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets path\n",
    "data_path = 'data' # NOTE: Change this path to the location of your BATADAL dataset files\n",
    "\n",
    "# Load benign train data\n",
    "train_benign_data = pd.read_csv(os.path.join(data_path, 'BATADAL_dataset03.csv')) # clean data\n",
    "\n",
    "# Load attack train data\n",
    "train_attack_data = pd.read_csv(os.path.join(data_path, 'BATADAL_dataset04.csv')) # data with attacks: 6-months long; attacks 1-7\n",
    "train_attack_data = train_attack_data.rename(\n",
    "                                columns={' L_T1': 'L_T1', ' L_T2': 'L_T2', ' L_T3': 'L_T3', ' L_T4': 'L_T4', ' L_T5': 'L_T5', ' L_T6': 'L_T6',\t' L_T7': 'L_T7',\n",
    "                                            ' P_J14': 'P_J14', ' P_J422': 'P_J422', ' P_J280': 'P_J280', ' P_J269': 'P_J269', ' P_J300': 'P_J300', ' P_J256': 'P_J256', ' P_J289': 'P_J289', ' P_J415': 'P_J415', ' P_J302': 'P_J302', ' P_J306': 'P_J306', ' P_J307': 'P_J307', ' P_J317': 'P_J317',\n",
    "                                            ' F_PU1': 'F_PU1', ' F_PU2': 'F_PU2', ' F_PU3': 'F_PU3', ' F_PU4': 'F_PU4', ' F_PU5': 'F_PU5', ' F_PU6': 'F_PU6', ' F_PU7': 'F_PU7', ' F_PU8': 'F_PU8', ' F_PU9': 'F_PU9', ' F_PU10': 'F_PU10', ' F_PU11': 'F_PU11', ' F_V2': 'F_V2',\n",
    "                                            ' S_PU1': 'S_PU1', ' S_PU2': 'S_PU2', ' S_PU3': 'S_PU3', ' S_PU4': 'S_PU4', ' S_PU5': 'S_PU5', ' S_PU6': 'S_PU6', ' S_PU7': 'S_PU7', ' S_PU8': 'S_PU8', ' S_PU9': 'S_PU9', ' S_PU10': 'S_PU10', ' S_PU11': 'S_PU11', ' S_V2': 'S_V2'})\n",
    "\n",
    "# Load test data - benign + attack data\n",
    "test_data = pd.read_csv(os.path.join(data_path, 'BATADAL_test_dataset.csv')) # data with attacks: 3-months long; attacks 8-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the DATETIME column and the features\n",
    "train_benign_datetime = train_benign_data['DATETIME']\n",
    "train_attack_datetime = train_attack_data['DATETIME']\n",
    "test_datetime = test_data['DATETIME']\n",
    "\n",
    "# Preprocessing: Scale data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_benign_scaled = scaler.fit_transform(train_benign_data.drop(columns=['DATETIME', 'ATT_FLAG']))\n",
    "train_attack_scaled = scaler.transform(train_attack_data.drop(columns=['DATETIME', ' ATT_FLAG']))\n",
    "test_scaled = scaler.transform(test_data.drop(columns=['DATETIME']))\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_benign_tensor = torch.tensor(train_benign_scaled, dtype=torch.float32)\n",
    "train_attack_tensor = torch.tensor(train_attack_scaled, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack intervals\n",
    "train_benign_attacks_time_intervals = []\n",
    "train_attack_attacks_time_intervals = [\n",
    "        {\"id\": 1, \"start\": \"13/09/2016 23\", \"end\": \"16/09/2016 00\"},    # Attacker changesL_T7 thresholds -->> low level in T7\n",
    "        {\"id\": 2, \"start\": \"26/09/2016 11\", \"end\": \"27/09/2016 10\"},    # Attacker changesL_T7 thresholds -->> T2 low level in T7\n",
    "        {\"id\": 3, \"start\": \"09/10/2016 09\", \"end\": \"11/10/2016 20\"},   # Attack alters L_T1 readings -->> pumps PU1/PU2 on stays on\n",
    "        {\"id\": 4, \"start\": \"29/10/2016 19\", \"end\": \"02/11/2016 16\"},   # Attack alters L_T1 readings -->> pumps PU1/PU2 on stays on\n",
    "        {\"id\": 5, \"start\": \"26/11/2016 17\", \"end\": \"29/11/2016 04\"},   # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "        {\"id\": 6, \"start\": \"06/12/2016 07\", \"end\": \"10/12/2016 04\"},   # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "        {\"id\": 7, \"start\": \"14/12/2016 15\", \"end\": \"19/12/2016 04\"},    # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "]\n",
    "test_attacks_time_intervals = [\n",
    "        {\"id\": 8, \"start\": \"16/01/2017 00\", \"end\": \"19/01/2017 06\"},    # Attacker changes L_T3 thresholds -->> low level in T3\n",
    "        {\"id\": 9, \"start\": \"30/01/2017 08\", \"end\": \"02/02/2017 00\"},    # Attacker changes L_T2 readings -->> T2 overflow\n",
    "        {\"id\": 10, \"start\": \"09/02/2017 03\", \"end\": \"10/02/2017 09\"},   # Malicious activation of pump PU3\n",
    "        {\"id\": 11, \"start\": \"12/02/2017 01\", \"end\": \"13/02/2017 07\"},   # Malicious activation of pump PU3\n",
    "        {\"id\": 12, \"start\": \"24/02/2017 05\", \"end\": \"28/02/2017 08\"},   # Attacker changes L_T2, F_V2 and S_V2, P_J14 and P_J422 -->> T2 overflow\n",
    "        {\"id\": 13, \"start\": \"10/03/2017 14\", \"end\": \"13/03/2017 21\"},   # Attacker changes L_T7 thresholds -->> PU10/PU11 are switching on/off continuously\n",
    "        {\"id\": 14, \"start\": \"25/03/2017 20\", \"end\": \"27/03/2017 01\"}    # Attacker changes L_T4 readings -->> T6 overflow\n",
    "]\n",
    "\n",
    "# Convert attack intervals to datetime objects\n",
    "train_benign_attacks_intervals = [\n",
    "    {\n",
    "        \"id\": interval[\"id\"],\n",
    "        \"start\": pd.to_datetime(interval[\"start\"], format=\"%d/%m/%Y %H\"),\n",
    "        \"end\": pd.to_datetime(interval[\"end\"], format=\"%d/%m/%Y %H\")\n",
    "    }\n",
    "    for interval in train_benign_attacks_time_intervals\n",
    "]\n",
    "train_attack_attacks_intervals = [\n",
    "    {\n",
    "        \"id\": interval[\"id\"],\n",
    "        \"start\": pd.to_datetime(interval[\"start\"], format=\"%d/%m/%Y %H\"),\n",
    "        \"end\": pd.to_datetime(interval[\"end\"], format=\"%d/%m/%Y %H\")\n",
    "    }\n",
    "    for interval in train_attack_attacks_time_intervals\n",
    "]\n",
    "test_attacks_intervals = [\n",
    "    {\n",
    "        \"id\": interval[\"id\"],\n",
    "        \"start\": pd.to_datetime(interval[\"start\"], format=\"%d/%m/%Y %H\"),\n",
    "        \"end\": pd.to_datetime(interval[\"end\"], format=\"%d/%m/%Y %H\")\n",
    "    }\n",
    "    for interval in test_attacks_time_intervals\n",
    "]\n",
    "\n",
    "# Attack indexes\n",
    "train_benign_attacks_index_intervals = []\n",
    "train_attack_attacks_index_intervals = [\n",
    "        {\"id\": 1, \"start\": 1727, \"end\": 1776},    # Attacker changesL_T7 thresholds -->> low level in T7\n",
    "        {\"id\": 2, \"start\": 2027, \"end\": 2050},    # Attacker changesL_T7 thresholds -->> T2 low level in T7\n",
    "        {\"id\": 3, \"start\": 2337, \"end\": 2396},    # Attack alters L_T1 readings -->> pumps PU1/PU2 on stays on\n",
    "        {\"id\": 4, \"start\": 2827, \"end\": 2920},    # Attack alters L_T1 readings -->> pumps PU1/PU2 on stays on\n",
    "        {\"id\": 5, \"start\": 3497, \"end\": 3556},    # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "        {\"id\": 6, \"start\": 3727, \"end\": 3820},    # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "        {\"id\": 7, \"start\": 3927, \"end\": 4036},    # Attacker reduces working speed of PU7 -->> lower water levels in T4\n",
    "]\n",
    "test_attacks_index_intervals = [\n",
    "        {\"id\": 8, \"start\": 297, \"end\": 366},    # Attacker changes L_T3 thresholds -->> low level in T3\n",
    "        {\"id\": 9, \"start\": 632, \"end\": 696},    # Attacker changes L_T2 readings -->> T2 overflow\n",
    "        {\"id\": 10, \"start\": 867, \"end\": 897},   # Malicious activation of pump PU3\n",
    "        {\"id\": 11, \"start\": 937, \"end\": 967},   # Malicious activation of pump PU3\n",
    "        {\"id\": 12, \"start\": 1229, \"end\": 1328},   # Attacker changes L_T2, F_V2 and S_V2, P_J14 and P_J422 -->> T2 overflow\n",
    "        {\"id\": 13, \"start\": 1574, \"end\": 1653},   # Attacker changes L_T7 thresholds -->> PU10/PU11 are switching on/off continuously\n",
    "        {\"id\": 14, \"start\": 1940, \"end\": 1969}    # Attacker changes L_T4 readings -->> T6 overflow\n",
    "]\n",
    "\n",
    "train_benign_attacks_indices  = np.zeros(train_benign_tensor.shape[0])\n",
    "for attack in train_benign_attacks_index_intervals:\n",
    "    train_benign_attacks_indices[attack['start']:attack['end']+1] = 1\n",
    "train_attack_attacks_indices  = np.zeros(train_attack_tensor.shape[0])\n",
    "for attack in train_attack_attacks_index_intervals:\n",
    "    train_attack_attacks_indices[attack['start']:attack['end']+1] = 1\n",
    "test_attacks_indices  = np.zeros(test_tensor.shape[0])\n",
    "for attack in test_attacks_index_intervals:\n",
    "    test_attacks_indices[attack['start']:attack['end']+1] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTkPbqGRynZb"
   },
   "source": [
    "# Classical Methods\n",
    "\n",
    "- Train with clean data, test on clean+anomalious data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_reconstruction_errors(attack_indices, attack_intervals, window_size, reconstruction_errors, sufix=\"\", plot_threshold=False, threshold=None):\n",
    "    # Plotting both reconstruction errors on a single plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    start_date = '2017-04-01 00:00:00'\n",
    "    hourly_datetime_list = pd.date_range(start=start_date, periods=len(attack_indices), freq='h')\n",
    "    if window_size > 1:\n",
    "        adjusted_indices = hourly_datetime_list[:-window_size+1] # Adjust indices to account for the window size - First-Point Strategy\n",
    "    else:\n",
    "        adjusted_indices = hourly_datetime_list\n",
    "    for (errors, label, color) in reconstruction_errors:\n",
    "        plt.plot(adjusted_indices, errors, label=label, color=color)\n",
    "\n",
    "    # Shading attack intervals\n",
    "    for interval in attack_intervals:\n",
    "        start_idx = interval[\"start\"]\n",
    "        end_idx = interval[\"end\"]\n",
    "\n",
    "        # Adjust the indices to account for the window size\n",
    "        if start_idx >= window_size and end_idx >= window_size:\n",
    "            adjusted_start_idx = start_idx - window_size\n",
    "            adjusted_end_idx = end_idx - window_size\n",
    "            plt.axvspan(adjusted_indices[adjusted_start_idx], adjusted_indices[adjusted_end_idx], color='red', alpha=0.3)\n",
    "        elif start_idx < window_size and end_idx >= window_size:\n",
    "            adjusted_start_idx = 0\n",
    "            adjusted_end_idx = end_idx - window_size\n",
    "            plt.axvspan(adjusted_indices[adjusted_start_idx], adjusted_indices[adjusted_end_idx], color='red', alpha=0.3)\n",
    "    \n",
    "    # Plot threshold\n",
    "    if plot_threshold:\n",
    "      plt.axhline(y=threshold, color='red', linestyle='--', label=\"Threshold\")\n",
    "\n",
    "    # Add labels, legend, and title\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Reconstruction Error')\n",
    "    plt.title(f\"Reconstruction Error Over Time{sufix}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAE76Qf7uktm"
   },
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1851,
     "status": "ok",
     "timestamp": 1747038078374,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "ez3LOfbmqE3S",
    "outputId": "60cf6fee-ec9d-4366-e81f-e9e98975eecc"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Train Isolation Forest on the training data\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=42)\n",
    "iso_forest.fit(train_benign_scaled)\n",
    "\n",
    "# Predict anomaly scores on the test data\n",
    "iso_forest_anomaly_scores = -iso_forest.decision_function(test_scaled) # Multiply by -1 to convert to anomaly scores (higher should mean more anomalous)\n",
    "\n",
    "# Convert anomaly scores to a pandas Series\n",
    "iso_forest_anomaly_scores_series = pd.Series(iso_forest_anomaly_scores)\n",
    "# Apply moving average smoothing (window size of 3, can be adjusted)\n",
    "iso_forest_smoothed_scores = iso_forest_anomaly_scores_series.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(iso_forest_smoothed_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(abs(iso_forest_smoothed_scores), f\"Isolation Forest\", \"green\")], \n",
    "                           sufix=f\" - Isolation Forest Anomaly Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x37BvDeyMMk"
   },
   "source": [
    "## One Class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1601,
     "status": "ok",
     "timestamp": 1747038084919,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "b6XNzmnJuwtj",
    "outputId": "4e7f9592-6d7d-4998-d1b0-1da66aa4c466"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Train One-Class SVM on the training data\n",
    "one_class_svm = OneClassSVM(kernel='rbf', gamma='scale', nu=0.05)\n",
    "one_class_svm.fit(train_benign_scaled)\n",
    "\n",
    "# Predict anomaly scores on the test data\n",
    "svm_scores = -one_class_svm.decision_function(test_scaled) # Multiply by -1 to convert to anomaly scores (higher should mean more anomalous)\n",
    "\n",
    "# Convert anomaly scores to a pandas Series\n",
    "svm_scores_series = pd.Series(svm_scores)\n",
    "# Apply moving average smoothing (window size of 3, can be adjusted)\n",
    "smoothed_svm_scores = svm_scores_series.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(smoothed_svm_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(abs(smoothed_svm_scores), f\"One-Class SVM\", \"green\")],\n",
    "                           sufix=f\" - One-Class SVM Anomaly Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS9PZ7espJMd"
   },
   "source": [
    "##  Local Outlier Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1747038090804,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "fYxrqpABpJXe",
    "outputId": "70d63cc3-c252-461c-ed8e-357cb14ee25f"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Train Local Outlier Factor on the training data\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination='auto', novelty=True)\n",
    "lof.fit(train_benign_scaled)\n",
    "\n",
    "# Predict anomaly scores on the test data\n",
    "lof_anomaly_scores = -lof.decision_function(test_scaled) # Multiply by -1 to convert to anomaly scores (higher should mean more anomalous)\n",
    "\n",
    "# Convert anomaly scores to a pandas Series\n",
    "lof_scores_series = pd.Series(lof_anomaly_scores)\n",
    "# Apply moving average smoothing (window size of 3, can be adjusted)\n",
    "smoothed_lof_scores = lof_scores_series.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(smoothed_lof_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(abs(smoothed_lof_scores), f\"Local Outlier Factor\", \"green\")],\n",
    "                           sufix = f\" - Local Outlier Factor Anomaly Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_l9GV5yXr3Un"
   },
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 489,
     "status": "ok",
     "timestamp": 1747038094885,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "bqA69Jc2r3cP",
    "outputId": "733b3bcd-2c39-4324-b3ad-35ece6a3017e"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train KNN on the training data\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
    "knn.fit(train_benign_scaled, np.zeros(len(train_benign_scaled)))\n",
    "\n",
    "# Predict anomaly scores on the test data\n",
    "distances, _ = knn.kneighbors(test_scaled)\n",
    "knn_scores = distances.mean(axis=1)\n",
    "\n",
    "# Convert anomaly scores to a pandas Series\n",
    "knn_scores_series = pd.Series(knn_scores)\n",
    "# Apply moving average smoothing (window size of 3, can be adjusted)\n",
    "smoothed_knn_scores = knn_scores_series.rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(smoothed_knn_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(abs(smoothed_knn_scores), f\"KNN\", \"green\")],\n",
    "                           sufix = f\" - KNN Anomaly Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LKdZLAgvzfJK"
   },
   "source": [
    "## Gaussian Mixture Models (GMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1747038100200,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "O4wQW1HAyPLC",
    "outputId": "d03d8d85-b907-4bb5-c620-43c859a5b5ac"
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Train Gaussian Mixture Model (GMM) on the training data\n",
    "gmm = GaussianMixture(n_components=2, covariance_type='full', random_state=42)\n",
    "gmm.fit(train_benign_scaled)\n",
    "\n",
    "# Compute log-likelihood of the test data to determine anomaly scores\n",
    "log_likelihood = gmm.score_samples(test_scaled)\n",
    "gmm_scores = -log_likelihood  # Higher negative log-likelihood indicates higher anomaly\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(gmm_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(abs(gmm_scores), f\"GMM\", \"green\")],\n",
    "                           sufix = f\" - GMM Anomaly Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAIErsFi78qW"
   },
   "source": [
    "## GAN-Based Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 138534,
     "status": "ok",
     "timestamp": 1747038251193,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "6NNwlxlo77Ue",
    "outputId": "0d5a65fc-effa-458b-ec59-55739c5ed80c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the generator model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Sigmoid()  # To generate data similar to the original range (0 to 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the discriminator model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # To classify real vs fake\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Parameters\n",
    "input_dim = train_benign_scaled.shape[1]\n",
    "latent_dim = 32\n",
    "batch_size = 64\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0002\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_benign_scaled, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Define DataLoader for training\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(latent_dim, input_dim).to(device)\n",
    "discriminator = Discriminator(input_dim).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "loss_function = nn.BCELoss()\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "real_labels = torch.ones(batch_size, 1).to(device)\n",
    "fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        # Train Discriminator\n",
    "        real_data = data[0].to(device)\n",
    "        batch_size_current = real_data.size(0)\n",
    "\n",
    "        # Create labels for current batch size (as the last batch may be smaller)\n",
    "        real_labels_current = torch.ones(batch_size_current, 1).to(device)\n",
    "        fake_labels_current = torch.zeros(batch_size_current, 1).to(device)\n",
    "\n",
    "        # Train with real data\n",
    "        outputs = discriminator(real_data)\n",
    "        d_loss_real = loss_function(outputs, real_labels_current)\n",
    "\n",
    "        # Train with fake data\n",
    "        z = torch.randn(batch_size_current, latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "        outputs = discriminator(fake_data.detach())\n",
    "        d_loss_fake = loss_function(outputs, fake_labels_current)\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # Backprop and optimize discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # Train Generator\n",
    "        z = torch.randn(batch_size_current, latent_dim).to(device)\n",
    "        fake_data = generator(z)\n",
    "        outputs = discriminator(fake_data)\n",
    "        g_loss = loss_function(outputs, real_labels_current)\n",
    "\n",
    "        # Backprop and optimize generator\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# Anomaly detection using reconstruction error\n",
    "GAN_anomaly_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in test_tensor:\n",
    "        # Generate a fake version of the input\n",
    "        x = x.to(device)\n",
    "        z = torch.randn(1, latent_dim).to(device)\n",
    "        generated_x = generator(z)\n",
    "\n",
    "        # Calculate anomaly score as reconstruction error (MSE between real and generated data)\n",
    "        anomaly_score = torch.mean((x - generated_x) ** 2).item()\n",
    "        GAN_anomaly_scores.append(anomaly_score)\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(GAN_anomaly_scores)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(GAN_anomaly_scores, f\"GAN\", \"green\")],\n",
    "                           sufix = f\" - GAN Anomaly Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9__7GZsknP8"
   },
   "source": [
    "## Mahalanobis Distance in Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 37003,
     "status": "ok",
     "timestamp": 1747038288199,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "Pi8xP0t5mw08",
    "outputId": "f61116e2-4c7f-4e29-f12f-da979be4f6a5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "# Define Autoencoder for Latent Space Representation\n",
    "class LatentAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=18):\n",
    "        super(LatentAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid()  # To ensure output values are between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Set up parameters\n",
    "input_dim = train_benign_scaled.shape[1]\n",
    "encoding_dim = 18\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "model = LatentAutoencoder(input_dim, encoding_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "train_tensor = torch.tensor(train_benign_scaled, dtype=torch.float32)\n",
    "test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Define DataLoader for training and testing\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training loop for Autoencoder\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        x = data[0].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        encoded_x, decoded_x = model(x)\n",
    "        loss = criterion(decoded_x, x)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Calculate mean and covariance of latent space for Mahalanobis distance\n",
    "model.eval()\n",
    "latent_representations = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        x = data[0].to(device)\n",
    "        encoded_x, _ = model(x)\n",
    "        latent_representations.append(encoded_x.cpu().numpy())\n",
    "\n",
    "# Combine all latent representations into a single array\n",
    "latent_representations = np.vstack(latent_representations)\n",
    "\n",
    "# Fit an empirical covariance model to the latent representations\n",
    "mean_vector = np.mean(latent_representations, axis=0)\n",
    "cov_model = EmpiricalCovariance().fit(latent_representations)\n",
    "\n",
    "# Function to compute Mahalanobis distance\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    diff = x - mean\n",
    "    distance = np.sqrt(np.dot(np.dot(diff, np.linalg.inv(cov)), diff.T))\n",
    "    return distance\n",
    "\n",
    "# Testing the Autoencoder on test data and calculating Mahalanobis distance in latent space\n",
    "mahalanobis_distances = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        x = data[0].to(device)\n",
    "        encoded_x, _ = model(x)\n",
    "        encoded_x = encoded_x.cpu().numpy()\n",
    "        for encoded_point in encoded_x:\n",
    "            dist = mahalanobis_distance(encoded_point, mean_vector, cov_model.covariance_)\n",
    "            mahalanobis_distances.append(dist)\n",
    "\n",
    "# Plot the anomaly scores over time using the test set\n",
    "adjusted_indices = list(range(len(mahalanobis_distances)))\n",
    "plot_reconstruction_errors(attack_indices=test_datetime, \n",
    "                           attack_intervals=test_attacks_index_intervals, \n",
    "                           window_size=1, \n",
    "                           reconstruction_errors=[(mahalanobis_distances, f\"Mahalanobis Distance\", \"green\")],\n",
    "                           sufix = f\" - Mahalanobis Distance in Latent Space\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7k6MuGmfyawh"
   },
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_segments(arr):\n",
    "  \"\"\"\n",
    "  Finds segments of consecutive True values in a boolean array.\n",
    "\n",
    "  Args:\n",
    "    arr: A boolean NumPy array.\n",
    "\n",
    "  Returns:\n",
    "    A list of tuples, where each tuple represents a segment and contains the\n",
    "    start and end indices (inclusive) of the segment.\n",
    "  \"\"\"\n",
    "  segments = []\n",
    "  start = -1\n",
    "  for i in range(len(arr)):\n",
    "    if arr[i] and start == -1:\n",
    "      start = i\n",
    "    elif not arr[i] and start != -1:\n",
    "      segments.append((start, i - 1, i - start))\n",
    "      start = -1\n",
    "  if start != -1:\n",
    "    segments.append((start, len(arr) - 1, len(arr)))\n",
    "  return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1747038053178,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "RW4B2_s-ym_1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics_scores(reconstruction_errors, test_ground_truth_labels, method_name):\n",
    "\n",
    "  # Calculate the best threshold to detect attacks using F1 score\n",
    "  precision, recall, thresholds = precision_recall_curve(test_ground_truth_labels, reconstruction_errors)\n",
    "  # Compute ROC curve\n",
    "  fpr, tpr, _ = roc_curve(test_ground_truth_labels, reconstruction_errors)\n",
    "  # Compute AUC (Area Under Curve)\n",
    "  roc_auc = auc(fpr, tpr)\n",
    "\n",
    "  # Calculate F1 score for each threshold\n",
    "  f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "  best_threshold_idx = np.argmax(f1_scores)\n",
    "  best_threshold = thresholds[best_threshold_idx]\n",
    "  best_f1_score = f1_scores[best_threshold_idx]\n",
    "\n",
    "  # Detect attacks using the best threshold\n",
    "  detected_attacks = reconstruction_errors > best_threshold\n",
    "\n",
    "  # TP\n",
    "  TP = np.sum((detected_attacks == 1) & (test_ground_truth_labels == 1))\n",
    "  # TN\n",
    "  TN = np.sum((detected_attacks == 0) & (test_ground_truth_labels == 0))\n",
    "  # FP\n",
    "  FP = np.sum((detected_attacks == 1) & (test_ground_truth_labels == 0))\n",
    "  # FN\n",
    "  FN = np.sum((detected_attacks == 0) & (test_ground_truth_labels == 1))\n",
    "\n",
    "  # TPR\n",
    "  TPR = TP/(TP+FN)\n",
    "  # TNR\n",
    "  TNR = TN/(FP+TN)\n",
    "\n",
    "  # Sclf\n",
    "  Sclf = (TPR + TNR)/2\n",
    "  # Sttd\n",
    "  Sttd = None\n",
    "\n",
    "  # Calculate the percentage of missed attack samples (False Negatives)\n",
    "  total_attack_samples = np.sum(test_ground_truth_labels == 1)\n",
    "  missed_attacks = np.sum((detected_attacks == 0) & (test_ground_truth_labels == 1))\n",
    "  missed_attack_percentage = (missed_attacks / total_attack_samples) * 100\n",
    "\n",
    "  # Calculate the percentage of wrongly detected samples (False Positives)\n",
    "  total_clean_samples = np.sum(test_ground_truth_labels == 0)\n",
    "  wrongly_detected = np.sum((detected_attacks == 1) & (test_ground_truth_labels == 0))\n",
    "  wrongly_detected_percentage = (wrongly_detected / total_clean_samples) * 100\n",
    "\n",
    "  # Add results to the table\n",
    "  new_row = {\n",
    "      \"num_of_detected\": 1,\n",
    "      \"TP\": TP,\n",
    "      \"FP\": FP,\n",
    "      \"TN\": TN,\n",
    "      \"FN\": FN,\n",
    "      \"TPR\": TPR,\n",
    "      \"TNR\": TNR,\n",
    "      \"S\": None, #y*Sttd+(1-y)*Sclf\n",
    "      \"Sttd\": Sttd,\n",
    "      \"Sclf\": Sclf,\n",
    "      \"total_attacks\": total_attack_samples,\n",
    "      \"total_clean\": total_clean_samples,\n",
    "      \"missed_attacks\": missed_attack_percentage,\n",
    "      \"wrongly_detected\": wrongly_detected_percentage,\n",
    "      \"accuracy\": (TP+TN)/(TP+TN+FP+FN),\n",
    "      \"f1_scores\": f1_scores,\n",
    "      \"best_f1_score\": best_f1_score,\n",
    "      \"best_threshold\": best_threshold,\n",
    "      \"precisuons\": precision,\n",
    "      \"recalls\": recall,\n",
    "      \"fpr\": fpr,\n",
    "      \"tpr\": tpr,\n",
    "      \"roc_auc\": roc_auc,\n",
    "  }\n",
    "  new_row_df = pd.DataFrame([new_row], index=[method_name])\n",
    "\n",
    "  return new_row_df, detected_attacks, precision, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 968
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1747038333484,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "quRSeDrjy46-",
    "outputId": "5f151bb7-7f85-4687-b9d6-76d2300c79e9"
   },
   "outputs": [],
   "source": [
    "iso_forest_results, iso_forest_detected_attacks, iso_forest_precision, iso_forest_recall = compute_metrics_scores(abs(iso_forest_smoothed_scores), test_attacks_indices, \"Isolation Forest\")\n",
    "svm_results, svm_detected_attacks, svm_precision, svm_recall = compute_metrics_scores(abs(smoothed_svm_scores), test_attacks_indices, \"One-Class SVM\")\n",
    "lof_results, lof_detected_attacks, lof_precision, lof_recall = compute_metrics_scores(abs(smoothed_lof_scores), test_attacks_indices, \"Local Outlier Factor\")\n",
    "knn_results, knn_detected_attacks, knn_precision, knn_recall = compute_metrics_scores(abs(smoothed_knn_scores), test_attacks_indices, \"KNN\")\n",
    "gmm_results, gmm_detected_attacks, gmm_precision, gmm_recall = compute_metrics_scores(abs(gmm_scores), test_attacks_indices, \"GMM\")\n",
    "GAN_anomaly_results, GAN_anomaly_detected_attacks, GAN_anomaly_precision, GAN_anomaly_recall = compute_metrics_scores(GAN_anomaly_scores, test_attacks_indices, \"GAN\")\n",
    "mahalanobis_distances_results, mahalanobis_distances_detected_attacks, mahalanobis_distances_precision, mahalanobis_distances_recall = compute_metrics_scores(mahalanobis_distances, test_attacks_indices, \"Mahalanobis Distance in Latent Space\")\n",
    "\n",
    "pd.concat([iso_forest_results,\n",
    "           svm_results,\n",
    "           lof_results,\n",
    "           knn_results,\n",
    "           gmm_results,\n",
    "           GAN_anomaly_results,\n",
    "           mahalanobis_distances_results\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2387,
     "status": "ok",
     "timestamp": 1743593865723,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "kbqybONI_Rrb",
    "outputId": "53a7c1f9-90ce-4b49-edde-04f953138440"
   },
   "outputs": [],
   "source": [
    "detected_segments_for_method = {}\n",
    "real_segments_for_method = {}\n",
    "removed_real_segments_for_method = {}\n",
    "\n",
    "for detected_attacks, method in [\n",
    "    (iso_forest_detected_attacks, 'iso_forest'),\n",
    "    (svm_detected_attacks, 'svm'),\n",
    "    (lof_detected_attacks, 'lof'),\n",
    "    (knn_detected_attacks, 'knn'),\n",
    "    (gmm_detected_attacks, 'gmm'),\n",
    "    (GAN_anomaly_detected_attacks, 'GAN_anomaly'),\n",
    "    (mahalanobis_distances_detected_attacks, 'mahalanobis_distances')\n",
    "]:\n",
    "\n",
    "  print(\"#\"*len(f'#####   {method}   #####'))\n",
    "  print(f\"#####   {method}   #####\")\n",
    "  print(\"#\"*len(f'#####   {method}   #####'))\n",
    "\n",
    "  detected_segments = find_segments(detected_attacks)\n",
    "  real_segments = find_segments(test_attacks_indices)\n",
    "  removed_real_segments_for_method[method] = 0\n",
    "  print(f\"Detected segments: {detected_segments}\")\n",
    "  print(f\"Real segments: {real_segments}\")\n",
    "\n",
    "  i = 0\n",
    "  j = 0\n",
    "  detected_segments_tmp = []\n",
    "  real_segments_tmp = []\n",
    "  while i<len(detected_segments) and j<len(real_segments):\n",
    "      if detected_segments[i][1] <= real_segments[j][0]:\n",
    "          i+=1\n",
    "      elif (detected_segments[i][0] < real_segments[j][1]) and (real_segments[j][0] < detected_segments[i][1]):\n",
    "          detected_segments_tmp.append(detected_segments[i])\n",
    "          real_segments_tmp.append(real_segments[j])\n",
    "          i+=1\n",
    "          j+=1\n",
    "          while i<len(detected_segments) and (detected_segments[i][1] < real_segments[j][0]):\n",
    "              i+=1\n",
    "          while j<len(real_segments) and (real_segments[j][1] < detected_segments[i][0]):\n",
    "              removed_real_segments_for_method[method]+=1\n",
    "              j+=1\n",
    "      elif real_segments[j][1] <= detected_segments[i][0] :\n",
    "          removed_real_segments_for_method[method]+=1\n",
    "          j+=1\n",
    "  detected_attacks_tmp  = np.zeros(detected_attacks.shape[0])\n",
    "  for attack in detected_segments_tmp:\n",
    "      detected_attacks_tmp[attack[0]:attack[1]+1] = 1\n",
    "  real_attacks_tmp  = np.zeros(test_attacks_indices.shape[0])\n",
    "  for attack in real_segments_tmp:\n",
    "          real_attacks_tmp[attack[0]:attack[1]+1] = 1\n",
    "\n",
    "  detected_segments_for_method[method] = detected_attacks_tmp\n",
    "  real_segments_for_method[method] = real_attacks_tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1747038333938,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "VZCmB9SFl4fR",
    "outputId": "5a7d4aff-d0b9-485c-9bbf-165c768244b0"
   },
   "outputs": [],
   "source": [
    "for detected_attacks_len, method, results in [\n",
    "    (len(find_segments(iso_forest_detected_attacks)), 'iso_forest', iso_forest_results),\n",
    "    (len(find_segments(svm_detected_attacks)), 'svm', svm_results),\n",
    "    (len(find_segments(lof_detected_attacks)), 'lof', lof_results),\n",
    "    (len(find_segments(knn_detected_attacks)), 'knn', knn_results),\n",
    "    (len(find_segments(gmm_detected_attacks)), 'gmm', gmm_results),\n",
    "    (len(find_segments(GAN_anomaly_detected_attacks)), 'GAN_anomaly', GAN_anomaly_results),\n",
    "    (len(find_segments(mahalanobis_distances_detected_attacks)), 'mahalanobis_distances', mahalanobis_distances_results)\n",
    "]:\n",
    "\n",
    "  print(\"#\"*len(f'#####   {method}   #####'))\n",
    "  print(f\"#####   {method}   #####\")\n",
    "  print(\"#\"*len(f'#####   {method}   #####'))\n",
    "\n",
    "  detected_segments = detected_segments_for_method[method]\n",
    "  real_segments = real_segments_for_method[method]\n",
    "  assert len(detected_segments) == len(real_segments), f\"Segments lists must have the same length [{len(detected_segments)}<>{len(real_segments)}].\"\n",
    "\n",
    "  results['num_of_detected'] = detected_attacks_len\n",
    "  \n",
    "  ttd = 0\n",
    "  for detected_segment, real_segment in zip(detected_segments, real_segments):\n",
    "    ttd = ttd + max((detected_segment[0] - real_segment[0]), 0)/real_segment[2]\n",
    "  ttd = ttd + removed_real_segments_for_method[method]\n",
    "  Sttd = 1-ttd/len(find_segments(test_attacks_indices))\n",
    "\n",
    "  gama = 0.5\n",
    "  Sclf = results['Sclf'].values[0]\n",
    "  S = gama * Sttd + (1-gama) * Sclf\n",
    "\n",
    "  print(f\"Sttd = {Sttd}, S = {S}\")\n",
    "  results['Sttd'] = Sttd\n",
    "  results['S'] = S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 53,
     "status": "ok",
     "timestamp": 1747038333976,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "bcxsKo2XVNPm",
    "outputId": "204a9508-49fc-4ab6-d876-5a3e30922b7a"
   },
   "outputs": [],
   "source": [
    "pd.concat([iso_forest_results,\n",
    "           svm_results,\n",
    "           lof_results,\n",
    "           knn_results,\n",
    "           gmm_results,\n",
    "           GAN_anomaly_results,\n",
    "           mahalanobis_distances_results\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 647,
     "status": "ok",
     "timestamp": 1747039630723,
     "user": {
      "displayName": "Katarina Milenković",
      "userId": "04065717320561587056"
     },
     "user_tz": -120
    },
    "id": "3mB2rq6zptU5",
    "outputId": "24b243f1-5ad1-4753-b121-6fa4c5e8419e"
   },
   "outputs": [],
   "source": [
    "# Plot the values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize\n",
    "f, ax = plt.subplots(1,figsize=(7,7))\n",
    "\n",
    "lw = 3\n",
    "font_size = 15\n",
    "\n",
    "ax.plot(knn_results[\"fpr\"].iloc[0], knn_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"KNN\"))\n",
    "ax.plot(mahalanobis_distances_results[\"fpr\"].iloc[0], mahalanobis_distances_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"Mahalanobis Distance\"))\n",
    "ax.plot(gmm_results[\"fpr\"].iloc[0], gmm_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"GMM\"))\n",
    "ax.plot(svm_results[\"fpr\"].iloc[0], svm_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"One-Class SVM\"))\n",
    "ax.plot(GAN_anomaly_results[\"fpr\"].iloc[0], GAN_anomaly_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"GAN\"))\n",
    "ax.plot(iso_forest_results[\"fpr\"].iloc[0], iso_forest_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"Isolation Forest\"))\n",
    "ax.plot(lof_results[\"fpr\"].iloc[0], lof_results[\"tpr\"].iloc[0], linestyle = None, linewidth=lw, label='{}'.format(\"LOF\"))\n",
    "\n",
    "# axes label size\n",
    "ax.set_xlabel(\"False Positive Rate\", fontsize=font_size)\n",
    "ax.set_ylabel(\"True Positive Rate\", fontsize=font_size)\n",
    "\n",
    "# Tick label size\n",
    "ax.tick_params(axis='x', labelsize=font_size)\n",
    "ax.tick_params(axis='y', labelsize=font_size)\n",
    "\n",
    "ax.set_title(\"ROC curves\")\n",
    "ax.legend(fontsize=font_size)\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPJXGDxsYoOOPb527l8wO0",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
